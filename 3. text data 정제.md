# Text data Preprocessing

- **nltk** library(Natural Language Toolkit)를 이용하여 Text Processing을 위한 전처리를 실습한다.


#### 1. 영어 문장 토큰화하기

>
> ```python
> # Test processing을 위해 nltk package 를 import!
> # !pip install nltk==3.4
> 
> import nltk
> ```
>
> 아래 명령어를 통해 download 대화상자를 열어 패키지를 다운로드 받아야 합니다.
> 인터넷 속도 저하 시 매우 오래 걸리므로, 패키지 설치 경로만 확인한 다음 \[ (nltk, downloaded) nltk_data.zip ] 의 파일들을 복사합니다.
> 경로 예시 : **"C:\Users\{컴퓨터 이름}\AppData\Roaming\nltk_data"**
> nltk_data 폴더 안에 corpora, taggers, tokenizers 폴더가 바로 위치하도록 복사해줘야 합니다.

```python
# nltk.download()  # 텍스트 데이터 처리를 위한 패키지 다운로더

# Download following packages
# Corpora : stopwords, wordnet
# Models : averaged_perceptron_tagger, maxnet_treebank_pos_tagger, punkt
```

##### 1) nltk.word_tokenize(sen)


```python
# 전처리하고자 하는 문장을 String 변수로 저장한다
sentence = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'

# 각 문장을 토큰화한 결과를 출력한다
nltk.word_tokenize(sentence)  # 문장을 '단어 수준에서' 토큰화해 출력한다 
```




    ['NLTK',
     'is',
     'a',
     'leading',
     'platform',
     'for',
     'building',
     'Python',
     'programs',
     'to',
     'work',
     'with',
     'human',
     'language',
     'data',
     '.',
     'It',
     'provides',
     'easy-to-use',
     'interfaces',
     'to',
     'over',
     '50',
     'corpora',
     'and',
     'lexical',
     'resources',
     'such',
     'as',
     'WordNet',
     ',',
     'along',
     'with',
     'a',
     'suite',
     'of',
     'text',
     'processing',
     'libraries',
     'for',
     'classification',
     ',',
     'tokenization',
     ',',
     'stemming',
     ',',
     'tagging',
     ',',
     'parsing',
     ',',
     'and',
     'semantic',
     'reasoning',
     ',',
     'wrappers',
     'for',
     'industrial-strength',
     'NLP',
     'libraries',
     ',',
     'and',
     'an',
     'active',
     'discussion',
     'forum',
     '.']



#### 2. 영어 문장 품사 태깅(POS tagging)하기

##### 1) nltk.pos_tag(tokens)


```python
# 각 문장을 토큰화한 후 품사를 태깅하여 결과를 출력한다
# part of speach

tokens = nltk.word_tokenize(sentence)  # 문장을 토큰화한다
nltk.pos_tag(tokens)  # 토큰화한 문장을 대상으로 품사를 태깅("POS" Tagging)하여 출력한다

# 튜플은 인덱스 기반으로 꺼낼 수 있음
nltk.pos_tag(tokens)[3]
```




    ('leading', 'VBG')



#### 3. Stopwords 제거하기

##### 1) 불용어 호출 : stopwords.word('english')


```python
# nltk 모듈에서 Stopwords를 직접 불러온다
from nltk.corpus import stopwords
```


```python
# 영어의 stopwords를 불러와 변수에 저장한다 (stopwords에 속하는 "단어" 리스트)
# 불용어들 호출 // 필요없는 단어

stopWords = stopwords.words('english') # 지원 언어 목록 : stopwords.fileids()

# How many stop words
print(len(stopWords))
print()

# stop words 출력
print(stopWords)

# 리스트

```

    179
    
    ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]



```python
print(tokens)
```

    ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']



##### 2) 불용어 제거 : list + for + if

```python
# 문장에서 stopwords 제거

result = []  # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다

for token in tokens:  # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다
    if token.lower() not in stopWords:  # 만약 소문자로 변환한 token이 stopWords 내에 없으면:
        result.append(token)  # token을 리스트에 더해준다

print(result)  # 결과를 출력한다
```

    ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'NLP', 'libraries', ',', 'active', 'discussion', 'forum', '.']



```python
# stopwords에 쉼표(,)와 마침표(.) 추가하여 다시 적용하기

stop_words = stopwords.words("english") # stop_words == list
stop_words.append(',')
stop_words.append('.')

result = []  # stopwords가 제거된 결과를 담기 위한 리스트를 생성한다

for token in tokens:  # for문을 통해 각각의 token이 stopwords인지 아닌지를 판별해 결과에 저장한다
    if token.lower() not in stop_words:  # 만약 소문자로 변환한 token이 stopWords 내에 없으면:
        result.append(token)  # token을 리스트에 첨부한다

print(result)  # 결과를 출력한다
```

    ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', 'provides', 'easy-to-use', 'interfaces', '50', 'corpora', 'lexical', 'resources', 'WordNet', 'along', 'suite', 'text', 'processing', 'libraries', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrappers', 'industrial-strength', 'NLP', 'libraries', 'active', 'discussion', 'forum']



#### 4. 영화 리뷰 데이터 전처리하기 - Lemmatizing
- Lemmatization : 단어의 형태소적 & 사전적 분석을 통해 파생적 의미를 제거하고, 어근에 기반하여 **기본 사전형인 lemma**를 찾는 것


```python
# WordNetLemmatizer 객체 생성
lemmatizer = nltk.wordnet.WordNetLemmatizer()
```


```python
# WordNetLemmatize는 더 정확한 분석을 위해 PoS 정보를 추가로 입력받을 수 있음 (n : 명사 v : 동사 a : 형용사 r : 부사)
# default == n(명사) 이므로 'cats', 'geese' 들은 기본명사형인 'cat','geese'로 결과가 출력됨
# 'ran'은 동사를 나타내는 PoS 정보인 'v'를 함께 입력해주어야 제대로 결과를 확인할 수 있음
# 'better'도 마찬가지로, '형용사(a)'라는 정보를 함께 입력해주어야 원형인 'good'을 제대로 출력해줌

print(lemmatizer.lemmatize("cats")) # lemmatize한 결과를 출력한다
print(lemmatizer.lemmatize("geese"))
```

    cat
    goose



```python
print(lemmatizer.lemmatize("better"))
print(lemmatizer.lemmatize("better", pos="a"))
```

    better
    good



```python
print(lemmatizer.lemmatize("ran"))
print(lemmatizer.lemmatize("ran", 'v'))
```

    ran
    run



```python
# Stopwords
stop_words = stopwords.words("english")
stop_words.append(',')
stop_words.append('.')

with open('moviereview.txt', 'r', encoding='utf-8') as file:  # 읽기 형식('r')로 지정하고 인코딩은 'utf-8'로 설정한다
    lines = file.readlines()  # readlines 함수로 텍스트 파일의 내용을 읽어 "리스트로 저장"한다

sentence = lines[1] 
tokens = nltk.word_tokenize(sentence)  

# for문을 통해 stopwords 제거와 lemmatization을 수행한다
lemmas = []  # lemmatize한 결과를 담기 위한 리스트를 생성한다
for token in tokens:  
    if token.lower() not in stop_words:  # 소문자로 변환한 token이 stopwords에 없으면:
        lemmas.append(lemmatizer.lemmatize(token))  # lemmatize한 결과를 리스트에 첨부한다

print('Lemmas of : ' + sentence)  # lemmatize한 결과를 출력한다
print(lemmas)
```

    Lemmas of : Despite being a huge comic book nerd I was not familiar with the Guardians before the first movie came out. I did some googling and upon learning about this sci-fi superhero team consisting of a talking raccoon and tree man I was hardly impressed.I finally got round to watching the first film and was blown away. It had a remarkable charm that even the other Marvel titles didn't have. It was filled with great humour, memorable moments and fit into the Marvel Universe long running story perfectly.Because of this I pumped by expectations up high for the sequel to a degree where it was almost guaranteed to fail yet somehow, someway it didn't.Not only did it meet my expectations but it exceeded them, GotG2 is amazing.Full of the same five star humour, being a visual treat and once again with an excellent soundtrack the film gripped me from the outset and delivered that charm all over again.This time including several industry veterans including Kurt Russell (Who has been on great form since his return) and Sylvester Stallone they fit in well and don't detract from the franchise as I feared they might.With cameo appearances along the way from the likes of Farscape (1999) lead Ben Browder to industry legend Seth Green as Howard the Duck this is a fun rollercoaster ride than left me positively gagging for more.This is one of those films I feel like I could rant about (In a positive way) for a while and so I'm going to resist the urge and merely say that Guardians 2 is a contender for the best Marvel movie, contested only by the first Avengers film.Masterpiece.The Good:Cast are great againCharm returnsExcellent nostalgic soundtrackLooks amazingThe Bad:Only one movie with Baby Groot? Noooo!They seem to have dropped Drax's taking everything literal jokes awayMichael Rosenbaum was wastedThings I Learnt From This Movie:Batista should have skipped over wrestling and just been an actor
    
    ['Despite', 'huge', 'comic', 'book', 'nerd', 'familiar', 'Guardians', 'first', 'movie', 'came', 'googling', 'upon', 'learning', 'sci-fi', 'superhero', 'team', 'consisting', 'talking', 'raccoon', 'tree', 'man', 'hardly', 'impressed.I', 'finally', 'got', 'round', 'watching', 'first', 'film', 'blown', 'away', 'remarkable', 'charm', 'even', 'Marvel', 'title', "n't", 'filled', 'great', 'humour', 'memorable', 'moment', 'fit', 'Marvel', 'Universe', 'long', 'running', 'story', 'perfectly.Because', 'pumped', 'expectation', 'high', 'sequel', 'degree', 'almost', 'guaranteed', 'fail', 'yet', 'somehow', 'someway', "didn't.Not", 'meet', 'expectation', 'exceeded', 'GotG2', 'amazing.Full', 'five', 'star', 'humour', 'visual', 'treat', 'excellent', 'soundtrack', 'film', 'gripped', 'outset', 'delivered', 'charm', 'again.This', 'time', 'including', 'several', 'industry', 'veteran', 'including', 'Kurt', 'Russell', '(', 'great', 'form', 'since', 'return', ')', 'Sylvester', 'Stallone', 'fit', 'well', "n't", 'detract', 'franchise', 'feared', 'might.With', 'cameo', 'appearance', 'along', 'way', 'like', 'Farscape', '(', '1999', ')', 'lead', 'Ben', 'Browder', 'industry', 'legend', 'Seth', 'Green', 'Howard', 'Duck', 'fun', 'rollercoaster', 'ride', 'left', 'positively', 'gagging', 'more.This', 'one', 'film', 'feel', 'like', 'could', 'rant', '(', 'positive', 'way', ')', "'m", 'going', 'resist', 'urge', 'merely', 'say', 'Guardians', '2', 'contender', 'best', 'Marvel', 'movie', 'contested', 'first', 'Avengers', 'film.Masterpiece.The', 'Good', ':', 'Cast', 'great', 'againCharm', 'returnsExcellent', 'nostalgic', 'soundtrackLooks', 'amazingThe', 'Bad', ':', 'one', 'movie', 'Baby', 'Groot', '?', 'Noooo', '!', 'seem', 'dropped', 'Drax', "'s", 'taking', 'everything', 'literal', 'joke', 'awayMichael', 'Rosenbaum', 'wastedThings', 'Learnt', 'Movie', ':', 'Batista', 'skipped', 'wrestling', 'actor']

